# vector_db.py (ë””ë²„ê¹… & ìˆ˜ì • ë²„ì „)

import pandas as pd
import numpy as np
import faiss
import pickle
import os
from sentence_transformers import SentenceTransformer
import sys
import logging

# ë¡œê¹… ì¤‘ë³µ ë°©ì§€
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VectorDB:
    _instance = None
    _lock = None
    
    def __new__(cls, model_name='dragonkue/snowflake-arctic-embed-l-v2.0-ko', cache_dir='vector_db_cache'):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
            cls._instance._is_building = False  # ë¹Œë“œ ì¤‘ í”Œë˜ê·¸
        return cls._instance
    
    def __init__(self, model_name='dragonkue/snowflake-arctic-embed-l-v2.0-ko', cache_dir='vector_db_cache'):
        # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
        if self._initialized:
            return
        
        # ë¹Œë“œ ì¤‘ì´ë©´ ëŒ€ê¸° (ì´ì¤‘ ì´ˆê¸°í™” ë°©ì§€)
        if self._is_building:
            print("â³ ë²¡í„°DB êµ¬ì¶•ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
            return
        
        self._is_building = True
        self._initialized = True
        self.model = None
        self.model_name = model_name
        self.cache_dir = cache_dir
        self.indexes = {}
        self.metadata = {}
        self.processed_categories = set()  # ì´ë¯¸ ì²˜ë¦¬ëœ ì¹´í…Œê³ ë¦¬ ì¶”ì 
        
        try:
            if self._check_cache():
                print("ğŸ“¦ ê¸°ì¡´ ë²¡í„°DB ìºì‹œ ë°œê²¬!")
                self._load_from_cache()
            else:
                print("ğŸ”¨ ë²¡í„°DB ìºì‹œ ì—†ìŒ â†’ ìƒˆë¡œ êµ¬ì¶• ì‹œì‘")
                self._build_from_scratch()
        finally:
            self._is_building = False
    
    
    def _check_cache(self):
        if not os.path.exists(self.cache_dir):
            return False
        
        required = ['CPU.index', 'GPU.index', 'RAM.index']
        for filename in required:
            if not os.path.exists(os.path.join(self.cache_dir, filename)):
                return False
        
        return True
    
    
    def _load_from_cache(self):
        print(f"ğŸ“‚ ìºì‹œ ë¡œë“œ ì¤‘: {self.cache_dir}/")
        
        for filename in os.listdir(self.cache_dir):
            if filename.endswith('.index'):
                category = filename.replace('.index', '')
                
                # ì´ë¯¸ ì²˜ë¦¬ëœ ì¹´í…Œê³ ë¦¬ëŠ” ìŠ¤í‚µ
                if category in self.processed_categories:
                    print(f"  â­ï¸  [{category}] ì´ë¯¸ ë¡œë“œë¨ (ìŠ¤í‚µ)")
                    continue
                
                index_path = os.path.join(self.cache_dir, filename)
                self.indexes[category] = faiss.read_index(index_path)
                
                pkl_path = os.path.join(self.cache_dir, f"{category}.pkl")
                with open(pkl_path, 'rb') as f:
                    self.metadata[category] = pickle.load(f)
                
                self.processed_categories.add(category)
                print(f"  âœ… [{category}] {len(self.metadata[category])}ê°œ")
        
        print("âœ… ë²¡í„°DB ë¡œë“œ ì™„ë£Œ!")
    
    
    def _build_from_scratch(self):
        print("ğŸ”¨ ë²¡í„°DB êµ¬ì¶• ì‹œì‘ (5~10ë¶„ ì†Œìš”)")
        
        print(f"  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        
        csv_files = {
            'CPU': 'ê°€ê³µë°ì´í„°/cpu.csv',
            'GPU': 'ê°€ê³µë°ì´í„°/gpu.csv',
            'RAM': 'ê°€ê³µë°ì´í„°/ram.csv',
            'SSD': 'ê°€ê³µë°ì´í„°/ssd.csv',
            'HDD': 'ê°€ê³µë°ì´í„°/hdd.csv',
            'MAINBORD': 'ê°€ê³µë°ì´í„°/mainbord.csv',
            'CASE': 'ê°€ê³µë°ì´í„°/case.csv',
            'POWER': 'ê°€ê³µë°ì´í„°/power.csv',
            'COOLER': 'ê°€ê³µë°ì´í„°/cooler.csv'
        }
        
        for category, filename in csv_files.items():
            # ì´ë¯¸ ì²˜ë¦¬ëœ ì¹´í…Œê³ ë¦¬ëŠ” ìŠ¤í‚µ
            if category in self.processed_categories:
                print(f"  â­ï¸  [{category}] ì´ë¯¸ ì²˜ë¦¬ë¨ (ìŠ¤í‚µ)")
                continue
            
            if os.path.exists(filename):
                self._process_csv(filename, category)
                self.processed_categories.add(category)
            else:
                print(f"  âš ï¸ íŒŒì¼ ì—†ìŒ: {filename}")
        
        self._save_to_cache()
        print("âœ… ë²¡í„°DB êµ¬ì¶• ë° ì €ì¥ ì™„ë£Œ!")
    
    
    def _process_csv(self, csv_path, category):
        """
        CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì„ë² ë”© ìƒì„± ë° ì¸ë±ìŠ¤ êµ¬ì¶•
        """
        print(f"\n[{category}] ì²˜ë¦¬ ì¤‘: {csv_path}")
        
        try:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            print(f"  ë°ì´í„°: {len(df)}ê°œ")
            
            # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì„ íƒ
            if 'í…ìŠ¤íŠ¸' in df.columns:
                texts = df['í…ìŠ¤íŠ¸'].astype(str).tolist()
            elif 'ì„ë² ë”©_í…ìŠ¤íŠ¸' in df.columns:
                texts = df['ì„ë² ë”©_í…ìŠ¤íŠ¸'].astype(str).tolist()
            else:
                texts = df['ì œí’ˆëª…'].astype(str).tolist()
            
            print(f"  ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.model.encode(texts, show_progress_bar=False)
            embeddings = np.array(embeddings).astype('float32')
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„±
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatIP(dimension)
            faiss.normalize_L2(embeddings)
            index.add(embeddings)
            
            # ê²°ê³¼ ì €ì¥
            self.indexes[category] = index
            self.metadata[category] = df.to_dict('records')
            
            print(f"  âœ… ì™„ë£Œ: {len(df)}ê°œ")
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    
    def _save_to_cache(self):
        os.makedirs(self.cache_dir, exist_ok=True)
        
        for category in self.indexes.keys():
            # ì´ë¯¸ ì €ì¥ëœ íŒŒì¼ì€ ìŠ¤í‚µ
            index_path = os.path.join(self.cache_dir, f"{category}.index")
            pkl_path = os.path.join(self.cache_dir, f"{category}.pkl")
            
            if os.path.exists(index_path) and os.path.exists(pkl_path):
                print(f"  â­ï¸  [{category}] ìºì‹œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬ (ìŠ¤í‚µ)")
                continue
            
            try:
                faiss.write_index(self.indexes[category], index_path)
                
                with open(pkl_path, 'wb') as f:
                    pickle.dump(self.metadata[category], f)
                
                print(f"  ğŸ’¾ [{category}] ìºì‹œ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ [{category}] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        print(f"ğŸ’¾ ì „ì²´ ìºì‹œ ì €ì¥ ì™„ë£Œ: {self.cache_dir}/")
    
    
    def search(self, category, query, top_k=20):
        category_key = None
        for key in self.indexes.keys():
            if key.lower() == category.lower():
                category_key = key
                break
        
        if not category_key:
            print(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì—†ìŒ: {category}")
            return []
        
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œë“œ
        if self.model is None:
            print(f"  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
        
        query_emb = self.model.encode([query])
        query_emb = np.array(query_emb).astype('float32')
        faiss.normalize_L2(query_emb)
        
        scores, indices = self.indexes[category_key].search(query_emb, top_k)
        
        results = []
        for idx, score in zip(indices[0], scores[0]):
            item = self.metadata[category_key][idx].copy()
            item['ìœ ì‚¬ë„'] = float(score)
            results.append(item)
        
        return results
    
    
    def get_categories(self):
        return list(self.indexes.keys())
    
    
    def get_stats(self):
        stats = {}
        for category in self.indexes.keys():
            stats[category] = len(self.metadata[category])
        return stats